import os
from dotenv import load_dotenv
from pymilvus import connections, Collection, utility
from langchain_together import ChatTogether, TogetherEmbeddings

# ğŸ”¹ Load API keys tá»« .env
load_dotenv()
ZILLIZ_URI = f"https://{os.getenv('ZILLIZ_HOST')}"
ZILLIZ_TOKEN = os.getenv("ZILLIZ_API_KEY")
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY_2")

# ğŸ”¹ Khá»Ÿi táº¡o mÃ´ hÃ¬nh chat tá»« Together AI
chat_model = ChatTogether(
    model="meta-llama/Llama-3.3-70B-Instruct-Turbo",
    together_api_key=TOGETHER_API_KEY
)

# ğŸ”¹ Khá»Ÿi táº¡o mÃ´ hÃ¬nh táº¡o embedding tá»« Together AI
embedding_model = TogetherEmbeddings(
    model="togethercomputer/m2-bert-80M-32k-retrieval",
    together_api_key=TOGETHER_API_KEY
)

# ğŸ”¹ Káº¿t ná»‘i Ä‘áº¿n Zilliz Cloud
print("ğŸ”„ Äang káº¿t ná»‘i Ä‘áº¿n Zilliz Cloud...")
connections.connect(alias="default", uri=ZILLIZ_URI, token=ZILLIZ_TOKEN)

# ğŸ”¹ Kiá»ƒm tra danh sÃ¡ch collections
collections = utility.list_collections()
if not collections:
    print("âš ï¸ KhÃ´ng cÃ³ collection nÃ o hoáº·c káº¿t ná»‘i khÃ´ng thÃ nh cÃ´ng!")
    exit()
print(f"âœ… Káº¿t ná»‘i thÃ nh cÃ´ng! Collections hiá»‡n cÃ³: {collections}")

# ğŸ”¹ Collection cáº§n kiá»ƒm tra
COLLECTION_NAME = "tthc_vectors_1"

if COLLECTION_NAME not in collections:
    print(f"âŒ Collection '{COLLECTION_NAME}' khÃ´ng tá»“n táº¡i!")
    exit()

# ğŸ”¹ Láº¥y collection
collection = Collection(COLLECTION_NAME)
collection.load()

# ğŸ”¹ Kiá»ƒm tra schema cá»§a collection
print("\nğŸ“Œ **Schema Collection:**")
print(collection.schema)

# ğŸ”¹ Kiá»ƒm tra sá»‘ lÆ°á»£ng báº£n ghi trong collection
num_entities = collection.num_entities
print(f"\nğŸ“Š **Sá»‘ lÆ°á»£ng báº£n ghi trong collection:** {num_entities}")

# ğŸ”¹ Láº¥y má»™t sá»‘ máº«u dá»¯ liá»‡u tá»« collection
print("\nğŸ“¦ **Láº¥y 5 báº£n ghi máº«u tá»« Zilliz:**")
sample_query = collection.query(
    expr="",
    output_fields=["id", "content", "intent", "section_name", "embedding"],
    limit=5
)

for idx, result in enumerate(sample_query, 1):
    print(f"\n=== ğŸ“ **TÃ€I LIá»†U {idx}** ===")
    print(f"ğŸ”¢ **ID:** {result.get('id', 'KhÃ´ng cÃ³')}")
    print(f"ğŸ“œ **Ná»™i dung (content):**\n{result.get('content', 'KhÃ´ng cÃ³')}")
    print(f"ğŸ“Œ **Intent:** {result.get('intent', 'KhÃ´ng cÃ³')}")
    print(f"ğŸ“Œ **Section Name:** {result.get('section_name', 'KhÃ´ng cÃ³')}")
    embedding = result.get("embedding", None)
    if embedding:
        print(f"ğŸ”¢ **Embedding:** {embedding[:10]}...")  # Chá»‰ in 10 giÃ¡ trá»‹ Ä‘áº§u cá»§a embedding
    else:
        print("âš ï¸ **Embedding khÃ´ng cÃ³ trong dá»¯ liá»‡u!**")
    print("=" * 50)

# ğŸ”¹ Kiá»ƒm tra API LLM cá»§a Together AI
def test_llm_api(query):
    """HÃ m Ä‘á»ƒ kiá»ƒm tra API LLM vá»›i má»™t truy váº¥n cá»¥ thá»ƒ."""
    try:
        response = chat_model.invoke(query)
        print("\nâœ… **Káº¿t quáº£ tá»« LLM:**", response)
    except Exception as e:
        print(f"âŒ **Lá»—i khi gá»i API LLM:** {str(e)}")

# ğŸ”¹ Táº¡o embedding vá»›i Together AI
def create_embedding(input_text):
    """HÃ m Ä‘á»ƒ táº¡o embedding cho má»™t Ä‘oáº¡n vÄƒn báº£n."""
    try:
        embedding = embedding_model.embed_query(input_text)
        print("\nâœ… **Embedding:**", embedding[:10])  # Chá»‰ in 10 giÃ¡ trá»‹ Ä‘áº§u Ä‘á»ƒ kiá»ƒm tra
        return embedding
    except Exception as e:
        print(f"âŒ **Lá»—i khi táº¡o embedding:** {str(e)}")
        return None

def ask_llm_based_on_docs(query, documents):
    """Há»i model nhÆ°ng chá»‰ cho phÃ©p nÃ³ tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u"""
    
    prompt = f"""
    DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c tÃ i liá»‡u liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n:

    {documents}

    Chá»‰ sá»­ dá»¥ng thÃ´ng tin tá»« tÃ i liá»‡u trÃªn Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i sau:
    "{query}"

    Náº¿u khÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i trong tÃ i liá»‡u, hÃ£y nÃ³i: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u."
    """

    # ğŸ”¹ Gá»i API cá»§a Together AI vá»›i prompt
    try:
        response = chat_model.invoke(prompt)
        return response
    except Exception as e:
        return f"âŒ Lá»—i khi gá»i model: {str(e)}"

def search_in_zilliz(query_text):
    """HÃ m tÃ¬m kiáº¿m tÃ i liá»‡u tÆ°Æ¡ng tá»± báº±ng vector search trong Zilliz"""
    print(f"\nğŸ” **TÃ¬m kiáº¿m tÃ i liá»‡u gáº§n nháº¥t vá»›i:** \"{query_text}\"")
    
    # ğŸ”¹ Láº¥y embedding cá»§a cÃ¢u há»i
    query_embedding = create_embedding(query_text)
    if query_embedding is None:
        print("âŒ KhÃ´ng thá»ƒ táº¡o embedding. Dá»«ng tÃ¬m kiáº¿m.")
        return

    # ğŸ”¹ Thá»±c hiá»‡n tÃ¬m kiáº¿m vector
    search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
    search_results = collection.search(
        data=[query_embedding],
        anns_field="embedding",
        param=search_params,
        limit=5,
        output_fields=["id", "content", "intent", "section_name"]
    )

    # ğŸ”¹ Kiá»ƒm tra xem cÃ³ káº¿t quáº£ tÃ¬m kiáº¿m khÃ´ng
    if not search_results or len(search_results[0]) == 0:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u phÃ¹ há»£p. Model sáº½ khÃ´ng tráº£ lá»i.")
        return "KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u liÃªn quan."

    # ğŸ”¹ Láº¥y ná»™i dung tÃ i liá»‡u tÃ¬m tháº¥y
    docs = []
    for idx, result in enumerate(search_results[0], 1):
        doc_content = result.entity.to_dict().get("content", "KhÃ´ng cÃ³ ná»™i dung")
        docs.append(doc_content)

    # ğŸ”¹ Káº¿t há»£p tÃ i liá»‡u vÃ o prompt Ä‘á»ƒ gá»­i cho model
    combined_docs = "\n\n".join(docs)
    return ask_llm_based_on_docs(query_text, combined_docs)

if __name__ == "__main__":
    test_query = "CÃ¡c bÆ°á»›c thá»±c hiá»‡n thá»§ tá»¥c Ä‘Äƒng kÃ½ khai tá»­?"
    
    # ğŸ”¹ Thá»±c hiá»‡n tÃ¬m kiáº¿m vÃ  chá»‰ tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u tÃ¬m tháº¥y
    answer = search_in_zilliz(test_query)
    print("\nâœ… **CÃ¢u tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u:**", answer)

